#!/bin/bash
set -x -e

cat > /home/hadoop/migrate.sh << 'EOF2'

function migrate {
    DIRNAME="$1"
    RESOLUTION="$2"
    CATALOGHOST="$3"
    TYPE="$4"

hadoop fs -mkdir -p /$TYPE/$DIRNAME
mkdir /home/hadoop/migrated/$DIRNAME
rm -f /home/hadoop/$DIRNAME
cat > /home/hadoop/$DIRNAME << 'EOFS1'
   { "type":"TYPE",
     "uri":"hdfs://TYPE/DIRNAME/FILENAME",
     "location":"HDFS",
     "resolution":"RESOLUTION",
     "filename":"FILENAME",
     "directory":"DIRNAME"}
EOFS1
for f in $(aws s3 ls net-openwhere-surge-resource-demsrcs/$DIRNAME/$TYPE/ | tr -s ' ' | cut -d " " -f 4)
do
      aws s3 cp s3://net-openwhere-surge-resource-demsrcs/$DIRNAME/$TYPE/$f /tmp/tif-file
      hadoop fs -put /tmp/tif-file /$TYPE/$DIRNAME/$f
      rm -f /tmp/tif-file
      touch /home/hadoop/migrated/$DIRNAME/$f
      rm -f /home/hadoop/mservice1
      cat /home/hadoop/$DIRNAME | sed s/FILENAME/$f/g | sed s/DIRNAME/$DIRNAME/g | sed s/TYPE/$TYPE/g | sed s/RESOLUTION/$RESOLUTION/g > /home/hadoop/mservice1
      curl -XPOST "http://$CATALOGHOST:8080/api/catalog/document" -d @/home/hadoop/mservice1 -H "Content-Type:application/json"
done
}

if [ ! -d "/home/hadoop/migrated" ]; then

NETWORK=`hdfs dfsadmin -report | grep Hostname: | cut -d \: -f 2 | cut -d \. -f 1 | sed -e "s/^[ \t]*//"`

# Wait for the network to register with resource manager
if [ -n "$NETWORK" ]; then

echo "Discovered network: $NETWORK"

#Run only on master
if grep isMaster /mnt/var/lib/info/instance.json | grep true;
then

if [ -f "/home/hadoop/accumulo/.initialized" ]; then

# prevent future runs by cron from doing anything
mkdir /home/hadoop/migrated

# wait 5 minutes for catalog to come up, it notices accumulo complete as well
sleep 300

INDEX=`curl -XGET "consul.surge.openwhere.net:8500/v1/catalog/service/catalog" | cut -d "," -f 2 | cut -d ":" -f 2 | sed 's/\"//g'`

migrate "va-1m" "1m" "$INDEX" "sources"
migrate "aster-30m" "30m" "$INDEX" "sources"
migrate "50cm" "50cm" "$INDEX" "bareearth"
migrate "50cm" "50cm" "$INDEX" "firstreturn"
migrate "50cm" "50cm" "$INDEX" "noobs"

cat > /home/hadoop/hdfs-done << 'EOFS3'
   { "type":"hdfsload",
     "status":"complete"}
EOFS3

curl -XPOST "http://$INDEX:8080/api/catalog/document" -d @/home/hadoop/hdfs-done -H "Content-Type:application/json"

fi

fi

fi

fi
EOF2

sudo sh -c "echo '*/5     * * * *   hadoop     bash /home/hadoop/migrate.sh > /home/hadoop/migrate.log 2>&1 ' >> /etc/crontab"
